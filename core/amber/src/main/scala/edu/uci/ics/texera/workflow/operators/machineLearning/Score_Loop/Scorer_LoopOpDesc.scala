package edu.uci.ics.texera.workflow.operators.machineLearning.Score_Loop

import com.fasterxml.jackson.annotation.{JsonProperty, JsonPropertyDescription}
import com.kjetland.jackson.jsonSchema.annotations.JsonSchemaTitle
import edu.uci.ics.amber.engine.common.workflow.{InputPort, OutputPort}
import edu.uci.ics.texera.workflow.common.metadata.annotations.AutofillAttributeName
import edu.uci.ics.texera.workflow.common.metadata.{OperatorGroupConstants, OperatorInfo}
import edu.uci.ics.texera.workflow.common.operators.PythonOperatorDescriptor
import edu.uci.ics.texera.workflow.common.tuple.schema.{Attribute, AttributeType, Schema}

class Scorer_LoopOpDesc extends PythonOperatorDescriptor {
  @JsonProperty(required = true)
  @JsonSchemaTitle("Actual Value")
  @JsonPropertyDescription("Specify the label column")
  @AutofillAttributeName
  var actualValue: String = ""

  @JsonProperty(required = true)
  @JsonSchemaTitle("Predicted Value")
  @JsonPropertyDescription("Specify the attribute generated by the model")
  @AutofillAttributeName
  var predictValue: String = ""


  @JsonProperty(required = true)
  @JsonSchemaTitle("Scorer Functions")
  @JsonPropertyDescription("Select multiple score functions")
  var scorers: List[Scorer_LoopFunction] = List()
  override def operatorInfo: OperatorInfo =
    OperatorInfo(
      "Scorer_Loop",
      "Scorer for machine learning classifier",
      OperatorGroupConstants.ML_GROUP,
      inputPorts = List(InputPort()),
      outputPorts = List(OutputPort())
    )

  override def generatePythonCode(): String = {
    val finalcode =
      s"""
         |from pytexera import *
         |import pandas as pd
         |import numpy as np
         |from sklearn.metrics import accuracy_score
         |from sklearn.metrics import precision_score
         |from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
         |import matplotlib.pyplot as plt
         |import plotly.express as px
         |import plotly.graph_objects as go
         |import plotly.io
         |
         |import json
         |
         |def drawConfusionMatrixImage(prediction, labels):
         |
         |  text = [[str(value) for value in row] for row in prediction]
         |
         |  fig = go.Figure(data=go.Heatmap(
         |    z=prediction,
         |    x=labels,
         |    y=labels,
         |    text=text,
         |    texttemplate="%{text}",
         |    hoverongaps=False,
         |    colorscale='Viridis',
         |    showscale=True))
         |
         |  fig.update_layout(
         |    title='Confusion Matrix',
         |    xaxis_title="Predicted Label",
         |    yaxis_title="True Label")
         |
         |  html = plotly.io.to_html(fig, include_plotlyjs='cdn', auto_play=False)
         |
         |  return html
         |
         |
         |class ProcessTableOperator(UDFTableOperator):
         |
         |    @overrides
         |    def process_table(self, table: Table, port: int) -> Iterator[Optional[TableLike]]:
         |        y_true = table["$actualValue"]
         |        y_p_list = table["$predictValue"][0]
         |        para = table["para"][0]
         |        model = table["model"][0]
         |        scorerList = [${getSelectedScorers()}]
         |        result = dict()
         |        for scorer in scorerList:
         |          result[scorer] = [None]*len(y_p_list)
         |
         |        for i in range(len(y_p_list)):
         |          for scorer in scorerList:
         |            y_pred = y_p_list[i]
         |            prediction = None
         |            if scorer == 'Accuracy':
         |              prediction = accuracy_score(y_true, y_pred)
         |              result['Accuracy'][i]=prediction
         |            elif scorer == 'Precision Score':
         |               prediction = precision_score(y_true, y_pred, average = 'macro')
         |               result['Precision Score'][i]=prediction
         |               # print('Precision Score', prediction)
         |            elif scorer == 'Confusion Matrix':
         |               column_set = set(y_true)
         |               labels = list(column_set)
         |               prediction = confusion_matrix(y_true, y_pred, labels = labels)
         |               # print('''Confusion Matrix
         |               # ''', prediction)
         |               prediction_json = json.dumps(prediction.tolist(), indent = 4)
         |               result['Confusion Matrix'][i]=prediction_json
         |
         |               html = drawConfusionMatrixImage(prediction,labels)
         |               result['Best Confusion Matrix Chart']=html
         |
         |        result["model"]=model
         |        result["para"] = para
         |        df = pd.DataFrame(result)
         |        yield df
         |""".stripMargin
    finalcode
  }
  override def getOutputSchema(schemas: Array[Schema]): Schema = {
    val outputSchemaBuilder = Schema.newBuilder
    scorers.map(scorer => getEachScorerName(scorer)).foreach(scorer =>
    {
      if (scorer == "Confusion Matrix") {
        outputSchemaBuilder.add(new Attribute(scorer, AttributeType.STRING))
        outputSchemaBuilder.add(new Attribute("Best Confusion Matrix Chart", AttributeType.STRING))
      } else {
        outputSchemaBuilder.add(new Attribute(scorer, AttributeType.DOUBLE))
      }
    }
    )
    outputSchemaBuilder.add(new Attribute("para", AttributeType.STRING))
    outputSchemaBuilder.add(new Attribute("model", AttributeType.BINARY)).build
  }


  private def getEachScorerName(scorer: Scorer_LoopFunction): String = {
    // Directly return the name of the scorer using the getName() method
    scorer.getName()
  }
  private def getSelectedScorers(): String = {
    // Return a string of scorers using the getEachScorerName() method
    scorers.map(scorer => getEachScorerName(scorer)).mkString("'", "','", "'")
  }


}